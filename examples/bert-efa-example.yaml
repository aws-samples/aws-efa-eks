apiVersion: kubeflow.org/v1alpha2
kind: MPIJob
metadata:
  name: bert-large-pretraining-efa
spec:
  slotsPerWorker: 8
  cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
         spec:
          imagePullPolicy: Always
          restartPolicy: OnFailure
          volumes:
          - name:  fsx
            persistentVolumeClaim:
              claimName: fsx-claim
          containers:
          - image: 231748552833.dkr.ecr.us-east-1.amazonaws.com/bert-efa-hvd-tf1-py3:latest
            name: bert-training-launcher
            env:
             - name: LD_LIBRARY_PATH
               value: /opt/amazon/openmpi/lib:/opt/nccl/build/lib:/opt/amazon/efa/lib:/opt/aws-ofi-nccl/install/lib:/usr/local/nvidia/lib:$LD_LIBRARY_PATH
             - name: PATH
               value: $PATH:/opt/amazon/efa/bin
             - name: XLA_FLAGS
               value: "--xla_gpu_cuda_data_dir=/usr/local/cuda"
             - name: TF_XLA_FLAGS
               value: "--tf_xla_cpu_global_jit"
            command:
            - /opt/amazon/openmpi/bin/mpirun
            - --allow-run-as-root
            - --tag-output
            - -np
            - "32"
            - -bind-to
            - none
            - -map-by
            - slot
            - -x
            - PATH
            - -x
            - LD_LIBRARY_PATH
            - -x
            - XLA_FLAGS
            - -x
            - TF_XLA_FLAGS
            - -x
            - NCCL_DEBUG=INFO
            - -x
            - NCCL_ALGO=RING
            - -x
            - FI_EFA_USE_DEVICE_RDMA=1
            - -x
            - RDMAV_FORK_SAFE=1
            - --mca
            - plm_rsh_no_tree_spawn
            - "1"
            - --mca
            - pml
            - ^cm
            - --oversubscribe
            - /usr/bin/python3
            - /workspace/bert/run_pretraining.py
            - --input_files_dir=/fsx/bert-tfrecords/tfrecord/lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training
            - --eval_files_dir=/fsx/bert-tfrecords/tfrecord/lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/test
            - --output_dir=/shared/checkpoints/phase_1
            - --bert_config_file=/fsx/bert-tfrecords/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json
            - --do_train=True
            - --do_eval=True 
            - --train_batch_size=64
            - --eval_batch_size=8 
            - --max_seq_length=128 
            - --max_predictions_per_seq=20 
            - --num_train_steps=1000 
            - --num_warmup_steps=2800 
            - --num_accumulation_steps=128 
            - --save_checkpoints_steps=7820 
            - --learning_rate=4.6875e-5 
            - --horovod 
            - --amp=false
            - --manual_fp16
            - --use_xla=true
            - --allreduce_post_accumulation=True
            volumeMounts:
            - name:  fsx
              mountPath: /fsx

    Worker:
      replicas: 4
      template:
        spec:
          imagePullPolicy: Always
          volumes:
          - name:  fsx
            persistentVolumeClaim:
              claimName: fsx-claim
          containers:
          - image: 231748552833.dkr.ecr.us-east-1.amazonaws.com/bert-efa-hvd-tf1-py3:latest
            name: bert-worker
            resources:
              limits:
                nvidia.com/gpu: 8
                hugepages-2Mi: 5120Mi
                memory: 384000Mi
                vpc.amazonaws.com/efa: 4
              requests:
                memory: 384000Mi
            volumeMounts:
            - name:  fsx
              mountPath: /fsx
